# DAPO: Data-Augmented Policy Optimization for Vision-Language Models

åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„å¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡GRPO (Group Relative Policy Optimization)ç®—æ³•è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œé‡ç‚¹å…³æ³¨æ³¨æ„åŠ›å¤šæ ·æ€§å’Œæ¨ç†è¿‡ç¨‹ä¼˜åŒ–ã€‚

## 1. DAPOè®­ç»ƒæ¡†æ¶

### æ ¸å¿ƒæ€æƒ³
DAPO (Data-Augmented Policy Optimization) æ˜¯ä¸€ç§ä¸“é—¨é’ˆå¯¹å¤šæ¨¡æ€æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡ä»¥ä¸‹æ–¹å¼ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ï¼š

- **æ³¨æ„åŠ›å¤šæ ·æ€§ä¼˜åŒ–**: é€šè¿‡MDI (Multi-modal Diversity Index) æŒ‡æ ‡ç›‘æ§å’Œä¼˜åŒ–æ³¨æ„åŠ›åˆ†å¸ƒ
- **æ¨ç†è¿‡ç¨‹å¼•å¯¼**: ä½¿ç”¨`<think></think>`æ ‡ç­¾å¼•å¯¼æ¨¡å‹è¿›è¡Œç»“æ„åŒ–æ¨ç†
- **å¤šé˜¶æ®µæ³¨æ„åŠ›åˆ†æ**: åˆ†åˆ«ç›‘æ§earlyã€middleã€lateé˜¶æ®µçš„æ³¨æ„åŠ›æ¨¡å¼

### è®­ç»ƒæµç¨‹
```
è¾“å…¥: å›¾åƒ + é—®é¢˜ â†’ æ¨¡å‹æ¨ç† â†’ è¾“å‡º: <think>æ¨ç†è¿‡ç¨‹</think>Answer: X
                â†“
        å¥–åŠ±è®¡ç®—: å‡†ç¡®æ€§ + æ ¼å¼ + æ³¨æ„åŠ›åˆ†å¸ƒ
                â†“
        DAPOä¼˜åŒ–: æ›´æ–°æ¨¡å‹å‚æ•°
```

## 2. å¥–åŠ±è®¡ç®—æœºåˆ¶

### 2.1 å¤šç»´åº¦å¥–åŠ±å‡½æ•°

**å‡†ç¡®æ€§å¥–åŠ± (Accuracy Reward)**
```python
accuracy_reward = 1.0 if predicted_answer == correct_answer else 0.0
```

**æ ¼å¼å¥–åŠ± (Format Reward)**
```python
# æ£€æŸ¥<think></think>æ ‡ç­¾æ ¼å¼
pattern = r"^<think>(?!.*<think>)(.*?)</think>.*$"
format_reward = 1.0 if re.match(pattern, completion) else 0.0
```

**æ³¨æ„åŠ›å¤šæ ·æ€§å¥–åŠ± (MDI Reward)**
```python
# åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„å¤šæ ·æ€§æŒ‡æ ‡
mdi_reward = calculate_mdi_penalty(attention_weights)
```

### 2.2 æ³¨æ„åŠ›ç›¸å…³Metricsè®¡ç®—

#### MDI (Multi-modal Diversity Index) è®¡ç®—
```python
def calculate_mdi(attention_weights, text_tokens, vision_tokens):
    """
    è®¡ç®—å¤šæ¨¡æ€å¤šæ ·æ€§æŒ‡æ•°
    """
    # 1. è®¡ç®—æ–‡æœ¬å’Œè§†è§‰tokençš„æ³¨æ„åŠ›åˆ†å¸ƒ
    text_attention = attention_weights[:, :len(text_tokens)]
    vision_attention = attention_weights[:, len(text_tokens):]
    
    # 2. è®¡ç®—æ³¨æ„åŠ›ç†µ
    text_entropy = -sum(p * log(p) for p in text_attention if p > 0)
    vision_entropy = -sum(p * log(p) for p in vision_attention if p > 0)
    
    # 3. è®¡ç®—MDIå€¼
    mdi = text_entropy / (text_entropy + vision_entropy + epsilon)
    return mdi
```

#### åˆ†é˜¶æ®µæ³¨æ„åŠ›åˆ†æ
```python
# Earlyé˜¶æ®µ (å‰1/3å±‚)
early_mdi = calculate_mdi(attention_weights[:num_layers//3])

# Middleé˜¶æ®µ (ä¸­é—´1/3å±‚)  
middle_mdi = calculate_mdi(attention_weights[num_layers//3:2*num_layers//3])

# Lateé˜¶æ®µ (å1/3å±‚)
late_mdi = calculate_mdi(attention_weights[2*num_layers//3:])

# æ•´ä½“MDI
all_mdi = calculate_mdi(attention_weights)
```

#### AEI (Attention Entropy Index) è®¡ç®—
```python
def calculate_aei(attention_weights, token_type):
    """
    è®¡ç®—æ³¨æ„åŠ›ç†µæŒ‡æ•°
    """
    if token_type == "text":
        attention = attention_weights[:, :num_text_tokens]
    else:  # vision
        attention = attention_weights[:, num_text_tokens:]
    
    # è®¡ç®—æ¯ä¸ªtokençš„æ³¨æ„åŠ›ç†µ
    token_entropies = []
    for token_idx in range(attention.shape[1]):
        token_attention = attention[:, token_idx]
        entropy = -sum(p * log(p) for p in token_attention if p > 0)
        token_entropies.append(entropy)
    
    return mean(token_entropies)
```

### 2.3 å¥–åŠ±æƒé‡å¹³è¡¡
```python
total_reward = (
    3.0 * accuracy_reward +      # å‡†ç¡®æ€§æƒé‡æœ€é«˜
    1.0 * format_reward +        # æ ¼å¼å¥–åŠ±
    mdi_penalty * mdi_reward     # æ³¨æ„åŠ›å¤šæ ·æ€§æƒ©ç½š
)
```

## 3. ä¸‰æ—¥å¿—è¾“å‡ºç³»ç»Ÿ

### 3.1 Rollout Results Log (`rollout_results.md`)

**æ€»ä½“ç»Ÿè®¡è¡¨æ ¼**
```markdown
| Metric | Value |
|--------|-------|
| Accuracy Mean | 0.750 Â± 0.212 |
| Format Mean | 0.850 Â± 0.212 |
| MDI Mean | 6.234 Â± 1.456 |
| Total Reward Mean | 2.400 Â± 0.848 |
| Success Rate | 0.750 |
```

**MDIç»Ÿè®¡è¡¨æ ¼**
```markdown
| Stage | MDI Mean | MDI Std | AEI Text | AEI Vision |
|-------|----------|---------|----------|------------|
| Early | 8.234 | 1.456 | 2.634 | 0.275 |
| Middle | 6.123 | 0.890 | 2.361 | 0.396 |
| Late | 4.567 | 0.678 | 2.238 | 0.450 |
| All | 6.234 | 1.456 | 2.411 | 0.374 |
```

**è¯¦ç»†æ ·æœ¬ä¿¡æ¯**
```json
{
  "rewards": {
    "accuracy": 1.0,
    "format": 1.0, 
    "mdi": 6.234,
    "total": 4.0
  },
  "attention_metrics": {
    "early": {"mdi": 8.234, "aei_text": 2.634, "aei_vision": 0.275},
    "middle": {"mdi": 6.123, "aei_text": 2.361, "aei_vision": 0.396},
    "late": {"mdi": 4.567, "aei_text": 2.238, "aei_vision": 0.450}
  },
  "token_counts": {
    "vision_tokens": 347,
    "text_tokens": 0,
    "total_tokens": 347
  }
}
```

### 3.2 Attention Diagnostics Log (`attention_diagnostics.md`)

**æ•´ä½“ç»Ÿè®¡ä¿¡æ¯**
```markdown
| Metric | Early | Middle | Late | All |
|--------|-------|--------|------|-----|
| MDI Mean | 8.234 | 6.123 | 4.567 | 6.234 |
| MDI Std | 1.456 | 0.890 | 0.678 | 1.456 |
| Vision Tokens | 347.0 Â± 0.0 |
| Text Tokens | 0.0 Â± 0.0 |
| Vision Ratio | 1.000 |
```

**åˆ†é˜¶æ®µè¯¦ç»†åˆ†æ**
```json
{
  "attention_metrics": {
    "early": {
      "mdi": 8.234,
      "aei_text": 2.634,
      "aei_vision": 0.275,
      "text_ratio": 0.915,
      "vision_ratio": 0.085
    },
    "middle": {
      "mdi": 6.123,
      "aei_text": 2.361,
      "aei_vision": 0.396,
      "text_ratio": 0.853,
      "vision_ratio": 0.147
    }
  }
}
```

### 3.3 Terminal Markdown Log (`run_terminal.md`)

**è®­ç»ƒè¿‡ç¨‹è®°å½•**
- æ¨¡å‹åŠ è½½å’Œé…ç½®ä¿¡æ¯
- è®­ç»ƒè¿›åº¦æ¡å’ŒæŒ‡æ ‡æ›´æ–°
- å®æ—¶MDIå€¼æ˜¾ç¤º
- é”™è¯¯å’Œè­¦å‘Šä¿¡æ¯

**å…³é”®è®­ç»ƒæŒ‡æ ‡**
```markdown
ğŸ“Š Logging 44 metrics for train mode
ğŸ¯ Reward Metrics:
   rewards/mc_idx_reward/mean: 0.7500
   rewards/think_format_reward/mean: 0.8500
ğŸ§  Attention Metrics:
   attention/early/mdi: 8.234
   attention/middle/mdi: 6.123
   attention/late/mdi: 4.567
```