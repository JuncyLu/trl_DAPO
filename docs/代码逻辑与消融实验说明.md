# 多模态 DAPO 训练框架：代码逻辑与消融实验说明

本文基于仓库源码进行全面梳理与复盘，按论文写作逻辑组织：方法、实现、数据、训练与损失、奖励设计、注意力度量、日志与可视化、消融实验，以及“token_weights”权重分配实验的设计与接入点。main.pdf 暂未解析，本文以源码为主进行说明。


**贡献概述**

- 在 TRL 的 GRPO 框架上定制 DAPOTrainer，支持多模态 VLM 的对话式强化学习训练，集成注意力诊断与 VGR 奖励。
- 构建多源数据集（A-OKVQA、VisRAG-ArxivQA、MMMU-Pro），标准化为统一的多选 QA 格式与 ChatML 提示。
- 设计组内分位数归一化的 VGR 奖励（含硬负样本变体），并提供奖励权重的早期阶段与常规阶段切换机制。
- 实现注意力度量（VGR 与 AEI）与 Markdown 训练日志，便于排查与分析。
- 实现“token 级 VGR 权重”的生成、注入与损失加权（全 token 加权），并提供日志与 Sanity Check 脚本。


**方法总览（DAPO for VLM）**

- 训练入口：`src/scripts/train_grpo_vlm.py:94`
  - 解析 `DAPOConfig`（训练参数）、`ModelConfig`（模型加载）、`ScriptArguments`（脚本级参数）。
  - 加载数据集 `lujunxi57/DDM` 并做图像 resize、RGB 转换；拼装 ChatML（system + user）。
  - 奖励函数按顺序注册：[accuracy, VGR/vgr_hard_negative, format, length]。
  - 实例化 `DAPOTrainer` 并启动训练与保存。

- 训练器：`src/trainer/dapo_trainer.py:120`（类 DAPOTrainer）
  - 负责一体化：生成（transformers 或 vLLM）、求对数概率与熵、计算优势与 KL、调用奖励函数、汇总奖励、日志与可视化、注意力量化、重放缓冲（replay buffer）以及 DAPO/GRPO 类损失。
  - 注意力诊断路径仅在常规（transformers.generate）路径开启输出注意力并计算指标。

- 配置：`src/trainer/dapo_config.py:21`（类 DAPOConfig）
  - 标准训练超参 + 本项目新增项：
    - 奖励权重与早期权重切换：`reward_weights`、`early_reward_weights`、`warmup_ratio`。
    - 注意力指标开关：`compute_attention_metrics=True`。
    - VGR 奖励变体开关：`vgr_hard_negative`。
    - 动态采样重放缓冲：`replay_buffer_size`、`filter_min_reward`、`replay_var_epsilon`。
    - 预留 token 级权重项：`token_weights`、`token_weights_topk_ratio`、`token_weights_clip`、`token_weights_smooth_sigma`（见 `src/trainer/dapo_config.py:685` 起）。


**数据与预处理**

- 数据构建脚本：`src/data/`
  - A-OKVQA 转换：`src/data/dataset_aokvqa.py:1`，支持 question_id 过滤，导出 parquet（image/problem/solution）。
  - VisRAG-ArxivQA 转换：`src/data/dataset_visrag_arxivqa.py:14`。
  - MMMU-Pro（4/10 选项）转换：`src/data/dataset_mmmupro.py:18`。
  - 合并为 DDM：`src/data/merge_parquet.py:22`。
  - 一键数据流水线示例：`data.sh`。

- 训练脚本中数据加载与规范化：`src/scripts/train_grpo_vlm.py:112`
  - `load_dataset("lujunxi57/DDM")`，按 9:1 切分。
  - 拼接 ChatML system 提示以约束输出格式（<think></think> 与 <answer></answer>）。
  - 图像 resize 到 1024 短边内、强制 RGB。


**生成与注意力度量**

- 生成分支三种：
  - vLLM server/colocate（不支持 attention 输出）；
  - transformers paged（不支持 attention 输出）；
  - 常规 transformers.generate（支持 attention 输出并计算指标）。

- 注意力收集与指标：
  - 生成阶段输出的 `attentions` 进入 `_process_attention_metrics`：`src/trainer/dapo_trainer.py:2611`。
  - 指标计算：`compute_qwen_attention_metrics_for_batch`：`src/utils/attention_metrics.py:339`。
    - 将注意力按层聚合、按 Query 的“生成阶段”行截取；
    - 构建“文本/视觉”Key 掩码：`_build_modality_masks`：`src/utils/attention_metrics.py:174`；
    - 分段 early/middle/late/all：`_split_layers_to_segments`：`src/utils/attention_metrics.py:237`；
    - 计算每段的 VGR 与 AEI：`_compute_segment_metrics`：`src/utils/attention_metrics.py:250`。
  - VGR 定义：`_compute_vgr`：`src/utils/attention_metrics.py:132`，以“注意力密度的比值”表征文本/视觉关注平衡；
  - AEI（Attention Efficiency Index）定义：`_compute_aei`：`src/utils/attention_metrics.py:150`，为注意力占比与 token 占比之比。
  - 评估用 VGR/AEI 复写到日志：`_log_attention_metrics`：`src/trainer/dapo_trainer.py:2660`。

- 真实 VGR 提取与日志：`compute_real_vgr_metrics`：`src/utils/dapo_logging.py:331`
  - 按 late 段优先、否则 all 段，导出 VGR/AEI 及 attention_text/attention_vision。
  - rollout/eval Markdown 写盘：`emit_rollout_logs`、`emit_eval_logs`（路径、字段见 `src/utils/dapo_logging.py`）。


**奖励函数与权重管理**

- 奖励函数（注册顺序即权重顺序）：
  - 准确率：`accuracy_reward`：`src/rewards/accuracy_rewards.py:12`，解析 <answer> 或关键行匹配，返回 {0,1}。
  - VGR 奖励：`vgr_reward`：`src/rewards/attention_rewards.py:18`，组内分位数归一化（IQR），VGR 越小越好，映射到 [0,1]。
  - VGR 硬负样本变体：`vgr_hard_negative`：`src/rewards/attention_rewards.py:157`，仅对 acc=1 的样本施加 [-0.5,0.5] 线性平移的奖惩，acc=0 为 0。
  - 格式：`think_format_reward`：`src/rewards/format_rewards.py:4`。
  - 长度惩罚：`get_soft_overlong_punishment/soft_overlong_punishment_reward`：`src/rewards/length_rewards.py:19,50`，在 `max_completion_length - soft_punish_cache` 后进入线性惩罚区，超长为 -1。

- 奖励权重与早期切换：
  - 管理器：`RewardWeightManager`：`src/rewards/reward_utils.py:20`；创建：`create_reward_weight_manager`。
  - 机制：若设定 `early_reward_weights`，暖启阶段或前 10% 步数使用 early，否则用常规 `reward_weights`：`update_weights`：`src/rewards/reward_utils.py:63`。
  - 训练中计算每个奖励并拼合：`_calculate_rewards` → `calculate_total_reward`：`src/trainer/dapo_trainer.py:1097,1733` 与 `src/rewards/reward_utils.py:93`。


**训练损失（DAPO/GRPO 系列）**

- 优势与比率：
  - 计算生成 token 的 `logps/entropies/KL`，支持引用模型与温度缩放、重要性采样、截断 IS（vLLM 校正）。
  - 支持 `importance_sampling_level` = token/sequence。

- 损失聚合：`_compute_loss`：`src/trainer/dapo_trainer.py:2189`
  - GRPO：按序列长度归一化。
  - BNPO：按本地 batch 有效 token 归一化。
  - DR-GRPO：按常数 `B*max_completion_length` 归一化。
  - DAPO（默认）：按“全局累计批次有效 token 数”归一化，消除长度偏置（对应论文 DAPO 提案）。

- 重放缓冲与替换：
  - 定义：`_ReplayBuffer`：`src/trainer/dapo_trainer.py:506`；启用条件：`replay_buffer_size>0`。
  - 仅训练态启用：组内标准差阈值 + 均值奖励过滤，按 `sum(|adv|)*std` 打分入队；对方差为 0 的组做替换：`src/trainer/dapo_trainer.py:1942` 起。


**token 级 VGR 权重（已接入损失）**

- 生成与注入：
  - 在 `_process_attention_metrics` 中调用 `build_token_vgr_weights_for_batch`（`src/utils/attention_token_weights.py:33`）计算每样本 token 权重；
  - 将最近一批权重缓存到 `self._logs["token_weights"]` 并在 `_generate_and_score_completions` 中右侧对齐后注入 `inputs["token_weights"] (B,T)`。

- 融入损失（全 token 加权）：
  - 在 `_compute_loss` 中使用 `adv_matrix = advantages.unsqueeze(1) * token_weights` 替代 `advantages.unsqueeze(1)`；
  - 完全移除旧的 Top‑K/非负优势门控逻辑，对所有 token 进行加权；
  - 参见：`src/trainer/dapo_trainer.py:2248` 附近。

- 日志：
  - 在与 rollout 相同 Markdown 文件追加“Token Weights”专章（均值/方差/min/max/比例与样本头部预览），函数 `emit_token_weights_logs`：`src/utils/dapo_logging.py`。
  - 逐步详细文件（output_dir/rollout_logs/rollout_step_*.md）已移除，仅保留总览型 rollout 日志，避免大 I/O 阻塞训练。
  - 新增"逐 token 注释"预览：对少量样本将 completion 的 token 与对应权重以 JSON 列表展示（无星号包围、不做可视化包裹），函数 `emit_tokenwise_weights`：`src/utils/dapo_logging.py`；
    - 输出文件：与 `--rollout_log_path` 同目录下的 `token_weights.md`（默认），如未指定 `rollout_log_path` 则落到 `training_logs/token_weights.md`。


**日志与可视化**

- rollout 明细日志：`emit_rollout_logs`：`src/utils/dapo_logging.py:14`，以 Markdown 输出：
  - Overall metrics：各奖励均值/方差、成功率；
  - 分段 VGR/AEI 摘要表；
  - 每样本明细（Prompt、Completion、Solution、奖励、详细注意力指标 JSON）。
- eval 日志：`emit_eval_logs`：`src/utils/dapo_logging.py:180` 起。
- 训练时机：生成与奖励计算完成后，在主进程写盘：`src/trainer/dapo_trainer.py:2098` 起。


**实验与配置（消融）**

- 通用设置（除非脚本另行覆盖）：
  - 模型：`Qwen/Qwen2.5-VL-7B-Instruct`；bfloat16；ZeRO-2；`max_prompt_length`（1024/2048）；`max_completion_length`（384/512）。
  - 采样：`num_generations=8`；每步日志 `logging_steps=1.0`；评估 `eval_steps=10`、`eval_num_generations=4`。
  - 优化：`lr=1e-5`、cosine、`warmup_ratio=0.05`、`max_grad_norm=1.0`。
  - 重放缓冲：`replay_buffer_size=64`、`replay_var_epsilon`（1e-6）、`filter_min_reward`（1.0/1.5/2.0 视脚本）。
  - 长度惩罚缓冲：`soft_punish_cache=50`。
  - 注意力指标与 Markdown 日志：默认启用。

- 实验 1（去除 VGR，强化 Accuracy）：`experiment1.sh`
  - 权重：`--reward_weights 2.5 0.0 0.5 1.0`，`--early_reward_weights 1.0 0.0 2.0 1.0`。
  - 解读：常规阶段强调准确率（2.5），VGR 权重为 0；早期阶段先偏向“格式/长度约束”。

- 实验 2（去除 Accuracy，仅保留 VGR）：`experiment2.sh`
  - 权重：`--reward_weights 0.0 2.5 0.5 1.0`，`--early_reward_weights 0.0 1.0 2.0 1.0`。
  - 解读：考察“仅 VGR”对模型注意力平衡与答案质量的影响。

- 实验 3（Accuracy + VGR 协同）：`experiment3.sh`
  - 权重：`--reward_weights 2.5 1.0 0.5 1.0`，`--early_reward_weights 1.0 1.0 2.0 1.0`。
  - 解读：主推设置，VGR 为加性项 `vgr_reward`，配合 accuracy/format/length 达到稳健训练。

- 实验 4（VGR 硬负样本）：`experiment4.sh`
  - 开启：`--vgr_hard_negative`；其余权重与实验 3 一致，`num_train_epochs=1`、`filter_min_reward=1.0`。
  - 解读：仅对 acc=1 的样本在组内做 VGR 归一化并赋予 [-0.5,0.5] 奖惩，acc=0 样本不受 VGR 奖惩影响，旨在稳定地“拉开好样本的注意力质量”。

- 通用训练脚本（含 LoRA 示例）：`dapo_train.sh`
  - 与实验 4 类似，但增加 `--use_peft --lora_target_modules "q_proj", "v_proj"` 以节省显存与加速迭代。

- 实验 5（仅 token_weights 验证）：`experiment5.sh`
  - 关键点：启用 `--token_weights`，同时将 VGR 奖励权重置 0（如 `--reward_weights 2.5 0.0 0.5 1.0`），用于隔离验证 token 加权的效果。


**实现注意事项与潜在改进**

- vLLM/transformers paged 生成路径不输出 attentions，VGR/AEI 与 token 权重将不可用；若需这些指标，请走常规 transformers.generate。
- 重放缓冲在多进程时做了归一与 gather，对齐逻辑较复杂，必要时可临时关闭以简化变量影响。
- 日志 Markdown 可能较大，建议按需调低 `prompt/completion_preview_chars`。


**复现实验的最小命令**

- 实验 1（无 VGR）：
  - `bash experiment1.sh`
- 实验 2（仅 VGR）：
  - `bash experiment2.sh`
- 实验 3（VGR+Acc）：
  - `bash experiment3.sh`
- 实验 4（VGR 硬负）：
  - `bash experiment4.sh`


**“token_weights”消融实验建议**

- 目标：验证“token 级 VGR 权重”用于放大“优势为正的关键 token”的训练信号，是否能进一步改善注意力分布与最终准确率。
- 变量：
  - `--token_weights` on/off；
  - `--token_weights_topk_ratio` ∈ {0.2, 0.4, 0.6}；
  - `--token_weights_smooth_sigma` ∈ {0, 1, 2}；
  - `--token_weights_clip` ∈ {(0.5,2.0), (0.2,3.0)}。
- 指标：
  - 训练收敛速度、评估准确率；
  - 注意力指标（late/all 段的 VGR/AEI）；
  - rollout “成功率/总奖励均值”。
- 建议基线：在实验 3 的设置上逐项开启/调整上述变量。


**文件索引与关键位置**

- 训练器与损失：`src/trainer/dapo_trainer.py:120, 2189, 2321, 2611`
- 训练配置：`src/trainer/dapo_config.py:21, 520, 685`
- 注意力度量与提取：`src/utils/attention_metrics.py:132, 150, 174, 237, 250, 339`
- token 权重构建：`src/utils/attention_token_weights.py:33`
- 日志与 VGR 摘要：`src/utils/dapo_logging.py:14, 331`
- 奖励函数：
  - accuracy：`src/rewards/accuracy_rewards.py:12`
  - VGR：`src/rewards/attention_rewards.py:18`
  - VGR 硬负：`src/rewards/attention_rewards.py:157`
  - format：`src/rewards/format_rewards.py:4`
  - length：`src/rewards/length_rewards.py:19, 50`
- 训练入口：`src/scripts/train_grpo_vlm.py:94`
- 消融脚本：`experiment1.sh`、`experiment2.sh`、`experiment3.sh`、`experiment4.sh`、`experiment5.sh`


—— 以上为源码导向的完整整理。若需，我可根据该文档直接提交 token 权重的最小接入补丁，或进一步补充实验记录模板（表格/曲线）。
